바로 정리해볼게.
말해준 5개 파일을 기준으로, **무슨 역할 + 안에 뭐가 들어가야 하는지**를 표로 싹 정리해줄게.

---

## 1. 파일별 상위 역할 요약

| 파일명                          | 역할 한 줄 요약                                                             |
| ---------------------------- | --------------------------------------------------------------------- |
| `configs.py`                 | 공통 하이퍼파라미터 및 경로 정의 (single source of truth)                           |
| `utils.py`                   | seed 고정, metric 계산, 로깅 세팅 등 공통 유틸 함수 모음                               |
| `models.py`                  | Equivariant GNN encoder/decoder + Residual MLP 정의                     |
| `train_encoder_qm9.py`       | QM9 구조-only self-supervised 학습 + latent/HOMO npz 덤프                   |
| `sampling_mlp_experiment.py` | latent 기반 샘플링 실험 (random vs k-center) + Residual MLP 학습/평가 + 곡선 저장/플롯 |

---

## 2. `configs.py` 명세

| 항목       | 내용                                                                                                                                                                                                                                                                                                                                                                                  |
| -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 주요 목적    | 하드코딩 방지, 공통 설정 한 곳에 관리 (모델/실험 스크립트에서 import)                                                                                                                                                                                                                                                                                                                                        |
| 주요 내용    | - 데이터/모델 저장 경로<br> - latent dim, hidden dim 등 모델 설정<br> - 학습 관련 설정(epochs, batch size, lr 등)<br> - 샘플링 실험에서 사용할 N 리스트 등                                                                                                                                                                                                                                                             |
| 필수 변수 예시 | - `DATA_ROOT = "..."` (QM9 root, 필요시)<br> - `LATENT_DIM = 128`<br> - `ENC_HIDDEN_IRREPS = "16x0e + 16x1o"` (e3nn용)<br> - `ENC_LR = 1e-3`, `ENC_BATCH_SIZE = 64`, `ENC_EPOCHS = 100`<br> - `MLP_HIDDEN_DIM = 256`, `MLP_LAYERS = 3`<br> - `MLP_LR = 1e-3`, `MLP_EPOCHS = 200`<br> - `TRAIN_VAL_TEST_SPLIT = (0.8, 0.1, 0.1)`<br> - `SAMPLING_NS = [10, 20, 50, 100, 200, 500, 1000]` |
| 기타       | - 나중에 MOF로 확장할 때도 그대로 재사용 가능하게 최대한 generic하게 이름 짓기                                                                                                                                                                                                                                                                                                                                  |

---

## 3. `utils.py` 명세

| 기능 범주       | 함수/내용                                                                                                          | 비고                                    |
| ----------- | -------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
| Seed 고정     | `def set_seed(seed: int):`<br>→ `random`, `numpy`, `torch`, `torch.cuda`까지 전부 고정                               | 재현성 확보 필수                             |
| Metric 계산   | `def mae(y_true, y_pred)`, `rmse(...)`, `r2_score_torch(...)` 또는 `r2_numpy(...)`                               | MLP 실험/로깅에 사용                         |
| 로깅 세팅       | `def get_logger(name: str, log_file: str = None) -> logging.Logger:`<br>→ `logging` + `RotatingFileHandler` 설정 | 모든 main 스크립트에서 사용                     |
| Device 헬퍼   | `def get_device() -> torch.device:`<br>→ `"cuda" if available else cpu`                                        | encoder/MLP 양쪽에서 공통 사용                |
| 데이터 분할      | `def train_val_test_split_indices(N, ratios, seed)`<br>→ `idx_train, idx_val, idx_test`                        | `train_encoder_qm9.py`에서 사용 후 npz에 저장 |
| 플롯 유틸(선택)   | `def save_learning_curve(xs, ys_dict, out_png)`<br>→ 샘플 수 vs MAE 플롯                                            | `sampling_mlp_experiment.py`에서 사용     |
| tqdm 래퍼(선택) | 필요시 단순 헬퍼 (없어도 됨)                                                                                              | 대신 각 스크립트에서 직접 `tqdm` 사용해도 OK         |

---

## 4. `models.py` 명세

### 4-1. Equivariant 구조 encoder

| 항목         | 내용                                                                                                                                                                                                                                                                                                 |
| ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 클래스명       | `class EquivGNNEncoder(nn.Module)`                                                                                                                                                                                                                                                                 |
| 입력         | - `z`: (N,) long, atomic numbers<br> - `pos`: (N, 3) float, 3D coordinates<br> - `batch`: (N,) long, PyG용 graph index                                                                                                                                                                              |
| 내부 구성 (예시) | - 원자 타입 embedding → e3nn Irreps("0e") scalar 특징<br> - PyG `radius_graph` or `knn_graph`로 edge 구성<br> - e3nn 기반 equivariant layer 여러 층 (TensorProduct, Spherical Harmonics 등)<br> - node-wise aggregation 후, batch-wise pooling(sum/mean)<br> - 최종적으로 `latent_dim` 차원의 graph-level latent vector 출력 |
| 출력         | - `z_graph`: (batch_size, latent_dim) float tensor                                                                                                                                                                                                                                                 |
| 기타         | - configs에서 `LATENT_DIM`, `ENC_HIDDEN_IRREPS` 등 가져오기<br> - 가능한 한 **inference 때 빠르게** 동작하도록 설계 (QM9 전체 latent 추출 시 중요)                                                                                                                                                                              |

### 4-2. 구조 Decoder (invariant)

| 항목             | 내용                                                                                                                               |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| 클래스명           | `class EquivDecoder(nn.Module)` (이름은 자유)                                                                                         |
| 입력             | - `z_graph`: (batch_size, latent_dim)                                                                                            |
| 출력 (재구성 타깃 예시) | - 예: 전역 descriptor 재구성<br>  - atom-type histogram<br>  - pairwise distance histogram (fixed bin size)<br>  - 간단한 RDF-like vector |
| 내부             | - MLP 기반 invariant decoder (`nn.Sequential`)로 충분<br> - 출력 차원은 선택한 descriptor dimension에 맞춤                                       |
| Loss에서 사용      | - `train_encoder_qm9.py`에서<br>  `loss_rec = MSE(decoder(z_graph), target_descriptor)` 형태로 구조-only self-supervised 학습             |
| 비고             | - 완전한 분자 재구성까지 안 가고, **distance/statistics 기반 재구성**만으로도 latent 구조화에 충분                                                           |

### 4-3. Residual 3층 MLP (HOMO용)

| 항목      | 내용                                                                                                                                                                                             |
| ------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 클래스명    | `class ResidualMLP(nn.Module)`                                                                                                                                                                 |
| 입력      | - `z`: (batch_size, latent_dim)                                                                                                                                                                |
| 내부      | - `in_proj`: Linear(latent_dim → hidden_dim)<br> - `num_layers`개의 residual block: <br>  `Linear(hidden_dim → hidden_dim)` + ReLU, 그리고 skip connection<br> - `out_proj`: Linear(hidden_dim → 1) |
| 출력      | - HOMO 예측값: (batch_size,) float                                                                                                                                                                |
| 사용 스크립트 | - 오직 `sampling_mlp_experiment.py`에서만 supervised로 학습                                                                                                                                            |
| 옵션      | - configs에서 `MLP_HIDDEN_DIM`, `MLP_LAYERS` 가져와 설정<br> - dropout, layer norm 등은 prototype에서는 생략해도 됨                                                                                             |

---

## 5. `train_encoder_qm9.py` 명세

| 항목         | 내용                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 주요 역할      | QM9 구조-only self-supervised 학습 + latent/HOMO npz 덤프                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| 주요 단계      | 1. argparse로 설정 받기 (epochs, batch_size, lr, latent_dim 등)<br> 2. logger 생성 (`utils.get_logger`)<br> 3. QM9 데이터 로딩(PyG `QM9` dataset 등)<br> 4. 구조 descriptor target 생성 (예: distance histogram 등)<br> 5. `EquivGNNEncoder` + `EquivDecoder` 초기화, optimizer 설정<br> 6. epochs 루프: train + (선택적) val loss 모니터링<br> 7. best encoder/decoder 저장 (`encoder_struct.pt`, `decoder_struct.pt`)<br> 8. best encoder 로드 후, 전체 QM9에 대해 latent z_all 추출<br> 9. QM9 raw 데이터에서 HOMO 값 y_all 추출 (학습에는 사용 X였음)<br> 10. train/val/test index 생성 (`utils.train_val_test_split_indices` or PyG의 split)<br> 11. `latents_qm9.npz`로 저장 |
| 학습 Loss 예시 | - `loss_rec = mse_loss(decoder(encoder(z)), target_descriptor)`<br> - 필요시 L2 regularization, KL-like term 등 추가 가능                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| I/O        | - 입력: QM9 데이터셋<br> - 출력 파일: <br>  - `encoder_struct.pt` (state_dict)<br>  - `decoder_struct.pt` (optional)<br>  - `latents_qm9.npz` (z_all, y_all, idx_train, idx_val, idx_test)                                                                                                                                                                                                                                                                                                                                                                                                                               |
| 로깅         | - epoch마다 train loss, val loss, 시간, learning rate 등 로깅<br> - tqdm으로 batch loop 진행 상황 표시                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |

---

## 6. `sampling_mlp_experiment.py` 명세

| 항목    | 내용                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ----- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 주요 역할 | latent 기반 샘플링 전략 비교 + Residual MLP 학습/평가 + 성능 곡선 저장/플롯                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| 주요 단계 | 1. argparse로 설정 받기 (샘플링 strategy, seed, output prefix 등)<br> 2. logger 생성<br> 3. `latents_qm9.npz` 로드 (`z_all, y_all, idx_train, idx_val, idx_test`)<br> 4. train part에서 다시 `z_train, y_train` 구성<br> 5. 두 전략 준비:<br>   - Random sampling<br>   - Latent-diversity sampling (`kcenter_greedy`)<br> 6. 샘플 사이즈 리스트 `Ns` (configs.SAMPLING_NS) 순회<br>   - 각 N에 대해:<br>      (a) subset index 선택<br>      (b) 해당 subset으로 ResidualMLP 새로 초기화 후 학습<br>      (c) 고정 test set에서 MAE/RMSE/R² 평가<br>      (d) 결과 리스트/CSV에 기록<br> 7. 전략별로 results CSV 저장<br> 8. Matplotlib으로 `N vs MAE` 곡선 플롯 (random vs latent) |
| 필요 함수 | - `kcenter_greedy(z_train, n_samples)`<br>  → latent-diversity subset 선택<br> - `train_mlp(z_train, y_train, z_val, y_val, configs)`<br>  → ResidualMLP 학습 loop (tqdm, logging 포함)<br> - `eval_mlp(model, z_test, y_test)`<br>  → MAE/RMSE/R² 계산                                                                                                                                                                                                                                                                                                                                                        |
| 결과 파일 | - `results_random.csv`<br> - `results_latent.csv`<br> - `curve_mae.png` (optional, random vs latent)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| 로깅    | - 각 N, 각 전략에서 test 성능 로깅<br> - seed, hyperparam도 로그에 남기면 나중에 재현 쉬움                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |

---

* 먼저 `configs.py` + `utils.py`를 짧게 구현
* 그다음 `models.py`에서 `ResidualMLP`부터 구현 (제일 쉬움)
* `EquivGNNEncoder`/Decoder는 e3nn/pyg 템플릿 보면서 천천히
* 마지막으로 `train_encoder_qm9.py` → `sampling_mlp_experiment.py` 순서로 가면 자연스러울 듯.
